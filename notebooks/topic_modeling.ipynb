{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/thinc/neural/train.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from .optimizers import Adam, SGD, linear_decay\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/thinc/check.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import defaultdict, Sequence, Sized, Iterable, Callable\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram, Stop-word Removal, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews_df.pkl', 'rb') as f:\n",
    "    reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_range</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>biz_stars</th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>real_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>-0DET7VdEQOJVJ_v6klEug</td>\n",
       "      <td>527 days 13:59:43</td>\n",
       "      <td>{'GoodForKids': 'True', 'WiFi': ''no'', 'Resta...</td>\n",
       "      <td>Asian Fusion, Restaurants</td>\n",
       "      <td>Markham</td>\n",
       "      <td>Flaming Kitchen</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ON</td>\n",
       "      <td>2017-05-02 00:33:10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This used  to be where Aka Teppan was. All the...</td>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>-0DET7VdEQOJVJ_v6klEug</td>\n",
       "      <td>527 days 13:59:43</td>\n",
       "      <td>{'GoodForKids': 'True', 'WiFi': ''no'', 'Resta...</td>\n",
       "      <td>Asian Fusion, Restaurants</td>\n",
       "      <td>Markham</td>\n",
       "      <td>Flaming Kitchen</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ON</td>\n",
       "      <td>2017-05-04 21:05:16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I like this place a lot better than Aka teppan...</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id      review_range  \\\n",
       "3187  -0DET7VdEQOJVJ_v6klEug 527 days 13:59:43   \n",
       "3188  -0DET7VdEQOJVJ_v6klEug 527 days 13:59:43   \n",
       "\n",
       "                                             attributes  \\\n",
       "3187  {'GoodForKids': 'True', 'WiFi': ''no'', 'Resta...   \n",
       "3188  {'GoodForKids': 'True', 'WiFi': ''no'', 'Resta...   \n",
       "\n",
       "                     categories     city             name  biz_stars state  \\\n",
       "3187  Asian Fusion, Restaurants  Markham  Flaming Kitchen        3.0    ON   \n",
       "3188  Asian Fusion, Restaurants  Markham  Flaming Kitchen        3.0    ON   \n",
       "\n",
       "                    date  review_stars  \\\n",
       "3187 2017-05-02 00:33:10           4.0   \n",
       "3188 2017-05-04 21:05:16           4.0   \n",
       "\n",
       "                                                   text  useful  real_counts  \n",
       "3187  This used  to be where Aka Teppan was. All the...      11          106  \n",
       "3188  I like this place a lot better than Aka teppan...       1          106  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This used  to be where Aka Teppan was. All the tables, chairs, cutlery, cups, and hot plates are the same. The menu however is much better. There's a lot of small snacks, drinks, dessert and food variety ! The menu has like 15 pages ! The food was also much tastier than it used to be. They even have a $40 wagu steak on the menu. I'm not sure how many people will be getting that, but I'm sure it's amazing. \\n\\nThe service wasn't bad either. The staff seem to be the same as the old restaurant. The service speed was not bad for a Monday and they were quite busy for soft opening. \\n\\nThe salty spicy fried squid balls and wasabi shake shake fries are a must try! This will def become one of my regular spots again.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.text[:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"If you've been to Aka Teppan (the predecessor at the same location) then you're in for a similar experience. The menu has been greatly expanded with newly enlarged selections of appetizers, drinks, and desserts. They also allow you to construct your own salad or sizzling plate. We started with Spicy Shake Shake Fries as an appetizer. It's a paper bag with fries and a shaker of spicy powder for you to combine. On the plus side, you can control exactly how spicy you want your fries. I had the Black Curry Beef Udon for my main. It arrived on a sizzling hot plate exactly as advertised. Make sure to flip your beef or it will stick to the plate. The noodles don't suffer from the same problem because they're covered in the curry sauce. I found the mild curry to be pleasant, but not memorable. I did manage to also try the Flaming Chicken Spicy Noodle which definitely lived up to its name.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['text'] = strip_newline(reviews.text)\n",
    "reviews.text[2:3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(sent_to_words(reviews.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'came', 'on', 'friday', 'night']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1111][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_trigrams(words, bi_min=5, tri_min=1, thresh=50):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min, threshold=thresh)\n",
    "    trigram = gensim.models.Phrases(bigram[words], min_count = tri_min, threshold=thresh)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_mod, trigram_mod = bigrams_trigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.phrases.Phraser"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seems to be doing a good job at picking up related food items: 'torched sashimi', 'roasted bone marrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'came', 'on', 'friday_night']\n"
     ]
    }
   ],
   "source": [
    "print(trigram_mod[bigram_mod[words[1111]]][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = remove_stopwords(reviews.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [trigram_mod[bigram_mod[review]] for review in no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "lemma = lemmatization(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note difference to un-lemmatized un-stop-worded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['come', 'friday_night']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma[1111][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary and Corpus creation for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(lemma)\n",
    "corpus = [id2word.doc2bow(text) for text in lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1111][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('also', 1),\n",
       " ('amazing', 1),\n",
       " ('bad', 1),\n",
       " ('even', 1),\n",
       " ('fry', 1),\n",
       " ('get', 1),\n",
       " ('hot', 2),\n",
       " ('lot', 1),\n",
       " ('menu', 1),\n",
       " ('plate', 3),\n",
       " ('table', 1),\n",
       " ('try', 1),\n",
       " ('experience', 1),\n",
       " ('location', 1),\n",
       " ('sauce', 1),\n",
       " ('side', 1),\n",
       " ('want', 1),\n",
       " ('finish', 1),\n",
       " ('go', 2),\n",
       " ('order', 1),\n",
       " ('come', 6),\n",
       " ('flavour', 1),\n",
       " ('great', 1),\n",
       " ('nice', 1),\n",
       " ('quickly', 1),\n",
       " ('touch', 1),\n",
       " ('fully', 1),\n",
       " ('meat', 2),\n",
       " ('quality', 2),\n",
       " ('back', 1),\n",
       " ('everything', 4),\n",
       " ('dish', 2),\n",
       " ('keep', 1),\n",
       " ('serve', 1),\n",
       " ('take', 1),\n",
       " ('well', 1),\n",
       " ('could', 1),\n",
       " ('little', 2),\n",
       " ('warm', 1),\n",
       " ('look', 1),\n",
       " ('overall', 1),\n",
       " ('star', 1),\n",
       " ('sort', 1),\n",
       " ('thing', 1),\n",
       " ('like', 1),\n",
       " ('think', 1),\n",
       " ('strange', 1),\n",
       " ('cool', 1),\n",
       " ('salmon', 2),\n",
       " ('wish', 1),\n",
       " ('favourite', 2),\n",
       " ('cold', 2),\n",
       " ('fresh', 1),\n",
       " ('weird', 1),\n",
       " ('ready', 1),\n",
       " ('appetitizer', 2),\n",
       " ('barely', 2),\n",
       " ('bread', 1),\n",
       " ('perfect', 3),\n",
       " ('show', 1),\n",
       " ('parking', 1),\n",
       " ('together', 2),\n",
       " ('second', 1),\n",
       " ('disappoint', 1),\n",
       " ('kind', 1),\n",
       " ('whole', 1),\n",
       " ('year', 1),\n",
       " ('friday_night', 1),\n",
       " ('stop', 1),\n",
       " ('work', 1),\n",
       " ('heavy', 1),\n",
       " ('wing', 2),\n",
       " ('reservation', 1),\n",
       " ('worry', 1),\n",
       " ('entr√©es', 2),\n",
       " ('solid', 1),\n",
       " ('duck', 2),\n",
       " ('moist', 1),\n",
       " ('dull', 1),\n",
       " ('beautifully', 1),\n",
       " ('dining', 2),\n",
       " ('hope', 1),\n",
       " ('follow', 1),\n",
       " ('delish', 1),\n",
       " ('crunchy', 1),\n",
       " ('book', 1),\n",
       " ('surface', 1),\n",
       " ('brunch', 1),\n",
       " ('fit', 1),\n",
       " ('roasted_bone_marrow', 2),\n",
       " ('happily', 1),\n",
       " ('lounge_area', 1),\n",
       " ('patio', 1),\n",
       " ('basil', 1),\n",
       " ('weather', 1),\n",
       " ('prepp', 1),\n",
       " ('smash', 1),\n",
       " ('torch', 1),\n",
       " ('beef_tartare', 1),\n",
       " ('lamb_sirloin', 2),\n",
       " ('puffed_wild_rice', 1),\n",
       " ('coke', 1),\n",
       " ('decline', 1),\n",
       " ('disturbed', 1),\n",
       " ('neccessarily', 1),\n",
       " ('represent', 1),\n",
       " ('sashimi', 1),\n",
       " ('torched_sashimi', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2word[id], freq) for id, freq in corpus[1111]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda50 = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=id2word,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=2)\n",
    "    lda50.save('lda50.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda50 = gensim.models.ldamulticore.LdaMulticore.load('lda50.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
